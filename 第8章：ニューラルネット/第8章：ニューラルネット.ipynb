{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1YwuFtZd1t",
   "metadata": {
    "editable": true,
    "id": "1e1YwuFtZd1t",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 第8章: ニューラルネット\n",
    "\n",
    "第7章で取り組んだポジネガ分類を題材として、ニューラルネットワークで分類モデルを実装する。なお、この章ではPyTorchやTensorFlow、JAXなどの深層学習フレームワークを活用せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff",
   "metadata": {
    "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff"
   },
   "source": [
    "## 70. 単語埋め込みの読み込み\n",
    "事前学習済み単語埋め込みを活用し、$|V| \\times d_{emb}$ の単語埋め込み行列$\\pmb{E}$を作成せよ。ここで、$|V|$は単語埋め込みの語彙数、$d_{emb}$は単語埋め込みの次元数である。ただし、単語埋め込み行列の先頭の行ベクトル$\\pmb{E}_{0,:}$は、将来的にパディング（`<PAD>`）トークンの埋め込みベクトルとして用いたいので、ゼロベクトルとして予約せよ。ゆえに、$\\pmb{E}$の2行目以降に事前学習済み単語埋め込みを読み込むことになる。\n",
    "\n",
    "もし、Google Newsデータセットの[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）を全て読み込んだ場合、$|V|=3000001, d_{emb}=300$になるはずである（ただ、300万単語の中には、殆ど用いられない稀な単語も含まれるので、語彙を削減した方がメモリの節約になる）。\n",
    "\n",
    "また、単語埋め込み行列の構築と同時に、単語埋め込み行列の各行のインデックス番号（トークンID）と、単語（トークン）への双方向の対応付けを保持せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9cce06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語埋め込み行列の形状: (3000001, 300)\n",
      "語彙数: 3000001\n",
      "埋め込み次元数: 300\n",
      "\n",
      "最初の5単語:\n",
      "ID: 1, 単語: </s>, ベクトル: [ 0.00112915 -0.00089645  0.00031853  0.00153351  0.00110626]...\n",
      "ID: 2, 単語: in, ベクトル: [0.0703125  0.08691406 0.08789062 0.0625     0.06933594]...\n",
      "ID: 3, 単語: for, ベクトル: [-0.01177979 -0.04736328  0.04467773  0.06347656 -0.01818848]...\n",
      "ID: 4, 単語: that, ベクトル: [-0.01574707 -0.02832031  0.08349609  0.05029297 -0.11035156]...\n",
      "ID: 5, 単語: is, ベクトル: [ 0.00704956 -0.07324219  0.171875    0.02258301 -0.1328125 ]...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    \"../第6章：単語ベクトル/GoogleNews-vectors-negative300.bin\", binary=True\n",
    ")\n",
    "\n",
    "vocab_size = len(model.key_to_index)\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "# 単語埋め込み行列を作成（語彙数+1 × 次元数）\n",
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "\n",
    "# 単語からIDへの辞書とIDから単語への辞書を作成\n",
    "word_to_id = {\"<PAD>\": 0}  # パディングトークンのIDは0\n",
    "id_to_word = {0: \"<PAD>\"}\n",
    "\n",
    "\n",
    "for i, word in enumerate(model.key_to_index, start=1):\n",
    "    embedding_matrix[i] = model[word]\n",
    "    word_to_id[word] = i\n",
    "    id_to_word[i] = word\n",
    "\n",
    "print(f\"単語埋め込み行列の形状: {embedding_matrix.shape}\")\n",
    "print(f\"語彙数: {len(word_to_id)}\")\n",
    "print(f\"埋め込み次元数: {embedding_matrix.shape[1]}\")\n",
    "\n",
    "print(\"\\n最初の5単語:\")\n",
    "for i in range(1, 6):\n",
    "    word = id_to_word[i]\n",
    "    print(f\"ID: {i}, 単語: {word}, ベクトル: {embedding_matrix[i][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e",
   "metadata": {
    "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e"
   },
   "source": [
    "## 71. データセットの読み込み\n",
    "\n",
    "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) をダウンロードし、訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、全てのテキストをトークンID列に変換せよ。このとき、単語埋め込みの語彙でカバーされていない単語は無視し、トークン列に含めないことにせよ。また、テキストの全トークンが単語埋め込みの語彙に含まれておらず、空のトークン列となってしまう事例は、訓練セットおよび開発セットから削除せよ（このため、第7章の実験で得られた正解率と比較できなくなることに注意せよ）。\n",
    "\n",
    "事例の表現方法は任意でよいが、例えば\"contains no wit , only labored gags\"がネガティブに分類される事例は、次のような辞書オブジェクトで表現すればよい。\n",
    "\n",
    "```\n",
    "{'text': 'contains no wit , only labored gags',\n",
    " 'label': tensor([0.]),\n",
    " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
    "```\n",
    "\n",
    "この例では、`text`はテキスト、`label`は分類ラベル（ポジティブなら`tensor([1.])`、ネガティブなら`tensor([0.])`）、`input_ids`はテキストのトークン列をID列で表現している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41053d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 66650, dev: 872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'contains no wit , only labored gags ',\n",
       " 'label': tensor([0.]),\n",
       " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_data(df, text_col_name, label_col_name):\n",
    "    dict_list = []\n",
    "    for text, label in zip(df[text_col_name], df[label_col_name]):\n",
    "        input_ids = [word_to_id[token] for token in text.split() if token in word_to_id]\n",
    "        if len(input_ids) > 0:\n",
    "            dict = {\n",
    "                \"text\": text,\n",
    "                \"label\": torch.tensor([float(label)]),\n",
    "                \"input_ids\": torch.tensor(input_ids),\n",
    "            }\n",
    "            dict_list.append(dict)\n",
    "    return dict_list\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(\"../第7章：機械学習/SST-2/train.tsv\", sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"../第7章：機械学習/SST-2/dev.tsv\", sep=\"\\t\")\n",
    "train_data = load_data(df_train, \"sentence\", \"label\")\n",
    "dev_data = load_data(df_dev, \"sentence\", \"label\")\n",
    "\n",
    "print(f\"train: {len(train_data)}, dev: {len(dev_data)}\")\n",
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca",
   "metadata": {
    "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca"
   },
   "source": [
    "## 72. Bag of wordsモデルの構築\n",
    "\n",
    "単語埋め込みの平均ベクトルでテキストの特徴ベクトルを表現し、重みベクトルとの内積でポジティブ及びネガティブを分類するニューラルネットワーク（ロジスティック回帰モデル）を設計せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8a2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "\n",
    "        # 通常の nn.Embedding はランダムなベクトルから始まるが、from_pretrainedを使うことで、すでにWord2Vecから作ったベクトルを使う\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=True,  # 訓練中にこのベクトルを更新しない\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids : 形状 (batch, seq_len) または (seq_len, )\n",
    "        \"\"\"\n",
    "        # 単語ID → 単語ベクトル\n",
    "        embeds = self.embedding(input_ids)\n",
    "\n",
    "        # 単語ベクトルの平均\n",
    "        if len(embeds.shape) == 2:\n",
    "            mean_embeds = embeds.mean(dim=0)  # 1文のとき  embeds.shape = (seq_len, embedding_dim)\n",
    "        else:\n",
    "            mean_embeds = embeds.mean(dim=1)  # ミニバッチの場合  embeds.shape = (batch, seq_len, embedding_dim)\n",
    "\n",
    "        # 線形変換（ロジスティック回帰）\n",
    "        logits = self.linear(mean_embeds)  # スカラー\n",
    "\n",
    "        return logits  # このまま loss 関数に渡せる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72385c44-ceab-4d62-a4df-3023e15a37e2",
   "metadata": {
    "id": "72385c44-ceab-4d62-a4df-3023e15a37e2"
   },
   "source": [
    "## 73. モデルの学習\n",
    "\n",
    "問題72で設計したモデルの重みベクトルを訓練セット上で学習せよ。ただし、学習中は単語埋め込み行列の値を固定せよ（単語埋め込み行列のファインチューニングは行わない）。また、学習時に損失値を表示するなど、学習の進捗状況をモニタリングできるようにせよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737bb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66650/66650 [01:17<00:00, 857.75it/s] \n",
      "Validation: 100%|██████████| 872/872 [00:00<00:00, 5911.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4026, Dev loss: 0.4591\n",
      "\n",
      "epoch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66650/66650 [01:24<00:00, 790.28it/s] \n",
      "Validation: 100%|██████████| 872/872 [00:00<00:00, 6246.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3726, Dev loss: 0.4528\n",
      "\n",
      "epoch3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66650/66650 [01:19<00:00, 843.64it/s] \n",
      "Validation: 100%|██████████| 872/872 [00:00<00:00, 5527.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3701, Dev loss: 0.4511\n",
      "\n",
      "epoch4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66650/66650 [01:18<00:00, 845.70it/s] \n",
      "Validation: 100%|██████████| 872/872 [00:00<00:00, 5414.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3692, Dev loss: 0.4505\n",
      "\n",
      "epoch5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66650/66650 [01:14<00:00, 900.48it/s] \n",
      "Validation: 100%|██████████| 872/872 [00:00<00:00, 5723.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3688, Dev loss: 0.4502\n",
      "\n",
      "epoch6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66650/66650 [01:19<00:00, 838.22it/s] \n",
      "Validation: 100%|██████████| 872/872 [00:00<00:00, 6063.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3686, Dev loss: 0.4500\n",
      "\n",
      "epoch7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66650/66650 [01:18<00:00, 853.56it/s] \n",
      "Validation: 100%|██████████| 872/872 [00:00<00:00, 5699.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3685, Dev loss: 0.4499\n",
      "\n",
      "epoch8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66650/66650 [01:19<00:00, 840.79it/s] \n",
      "Validation: 100%|██████████| 872/872 [00:00<00:00, 6120.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3684, Dev loss: 0.4498\n",
      "\n",
      "epoch9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66650/66650 [01:11<00:00, 936.39it/s] \n",
      "Validation: 100%|██████████| 872/872 [00:00<00:00, 5635.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3684, Dev loss: 0.4497\n",
      "\n",
      "epoch10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66650/66650 [01:16<00:00, 869.55it/s] \n",
      "Validation: 100%|██████████| 872/872 [00:00<00:00, 6151.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3683, Dev loss: 0.4497\n",
      "\n",
      "モデルを保存しました: model/bow_classifier_73.pth\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_data(df, text_col_name, label_col_name):\n",
    "    dict_list = []\n",
    "    for text, label in zip(df[text_col_name], df[label_col_name]):\n",
    "        input_ids = [word_to_id[token] for token in text.split() if token in word_to_id]\n",
    "        if len(input_ids) > 0:\n",
    "            dict = {\n",
    "                \"text\": text,\n",
    "                \"label\": torch.tensor([float(label)]),\n",
    "                \"input_ids\": torch.tensor(input_ids),\n",
    "            }\n",
    "            dict_list.append(dict)\n",
    "    return dict_list\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "\n",
    "        # 通常の nn.Embedding はランダムなベクトルから始まるが、from_pretrainedを使うことで、すでにWord2Vecから作ったベクトルを使う\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=True,  # 訓練中にこのベクトルを更新しない\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids : 形状 (batch, seq_len) または (seq_len, )\n",
    "        \"\"\"\n",
    "        # 単語ID → 単語ベクトル\n",
    "        embeds = self.embedding(input_ids)\n",
    "\n",
    "        # 単語ベクトルの平均\n",
    "        if len(embeds.shape) == 2:\n",
    "            mean_embeds = embeds.mean(dim=0)  # 1文のとき  embeds.shape = (seq_len, embedding_dim)\n",
    "        else:\n",
    "            mean_embeds = embeds.mean(dim=1)  # ミニバッチの場合  embeds.shape = (batch, seq_len, embedding_dim)\n",
    "\n",
    "        # 線形変換（ロジスティック回帰）\n",
    "        logits = self.linear(mean_embeds)  # スカラー\n",
    "\n",
    "        return logits  # このまま loss 関数に渡せる\n",
    "\n",
    "\n",
    "# 学習関数（非バッチ処理）\n",
    "def train_model(model, train_data, dev_data, criterion, optimizer, device):\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    for sample in tqdm(train_data, desc=\"Training\", leave=True):\n",
    "        input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)  # (1, seq_len)\n",
    "        label = sample[\"label\"].to(device)  # (1, )\n",
    "\n",
    "        optimizer.zero_grad()  # 勾配初期化\n",
    "        logits = model(input_ids)  # 順伝播\n",
    "        loss = criterion(logits.view(-1), label)  # 損失計算\n",
    "        loss.backward()  # 逆伝播\n",
    "        optimizer.step()  # パラメータ更新\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    dev_losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(dev_data, desc=\"Validation\", leave=True):\n",
    "            input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "            label = sample[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits.view(-1), label)\n",
    "            dev_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    dev_loss = np.mean(dev_losses)\n",
    "\n",
    "    return train_loss, dev_loss\n",
    "\n",
    "\n",
    "\n",
    "# 単語埋め込みの読み込み\n",
    "model = KeyedVectors.load_word2vec_format(\"../第6章：単語ベクトル/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "vocab_size = len(model.key_to_index)\n",
    "embedding_dim = model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "word_to_id = {\"<PAD>\": 0} \n",
    "id_to_word = {0: \"<PAD>\"}\n",
    "\n",
    "for i, word in enumerate(model.key_to_index, start=1):\n",
    "    embedding_matrix[i] = model[word]\n",
    "    word_to_id[word] = i\n",
    "    id_to_word[i] = word\n",
    "\n",
    "# データセット読み込み\n",
    "df_train = pd.read_csv(\"../第7章：機械学習/SST-2/train.tsv\", sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"../第7章：機械学習/SST-2/dev.tsv\", sep=\"\\t\")\n",
    "train_data = load_data(df_train, \"sentence\", \"label\")\n",
    "dev_data = load_data(df_dev, \"sentence\", \"label\")\n",
    "\n",
    "# ハイパーパラメータ・デバイス設定\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# モデル・損失関数・最適化手法の定義\n",
    "model = BoWClassifier(embedding_matrix).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "parameters = model.linear.parameters()  # 単語埋め込みはfreeze=Trueなので、線形層だけが学習対象\n",
    "optimizer = optim.Adam(parameters, lr=learning_rate)\n",
    "\n",
    "# 学習\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch{epoch+1}\")\n",
    "    train_loss, dev_loss = train_model(model, train_data, dev_data, criterion, optimizer, device)\n",
    "    print(f\"---> Train loss: {train_loss:.4f}, Dev loss: {dev_loss:.4f}\\n\")\n",
    "\n",
    "# モデル保存\n",
    "save_path = \"model/bow_classifier_73.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"モデルを保存しました: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f",
   "metadata": {
    "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f"
   },
   "source": [
    "## 74. モデルの評価\n",
    "\n",
    "問題73で学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9765e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開発セットにおける正解率：0.7982\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def load_data(df, text_col_name, label_col_name):\n",
    "    dict_list = []\n",
    "    for text, label in zip(df[text_col_name], df[label_col_name]):\n",
    "        input_ids = [word_to_id[token] for token in text.split() if token in word_to_id]\n",
    "        if len(input_ids) > 0:\n",
    "            dict = {\n",
    "                \"text\": text,\n",
    "                \"label\": torch.tensor([float(label)]),\n",
    "                \"input_ids\": torch.tensor(input_ids),\n",
    "            }\n",
    "            dict_list.append(dict)\n",
    "    return dict_list\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "\n",
    "        # 通常の nn.Embedding はランダムなベクトルから始まるが、from_pretrainedを使うことで、すでにWord2Vecから作ったベクトルを使う\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=True,  # 訓練中にこのベクトルを更新しない\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids : 形状 (batch, seq_len) または (seq_len, )\n",
    "        \"\"\"\n",
    "        # 単語ID → 単語ベクトル\n",
    "        embeds = self.embedding(input_ids)\n",
    "\n",
    "        # 単語ベクトルの平均\n",
    "        if len(embeds.shape) == 2:\n",
    "            mean_embeds = embeds.mean(dim=0)  # 1文のとき  embeds.shape = (seq_len, embedding_dim)\n",
    "        else:\n",
    "            mean_embeds = embeds.mean(dim=1)  # ミニバッチの場合  embeds.shape = (batch, seq_len, embedding_dim)\n",
    "\n",
    "        # 線形変換（ロジスティック回帰）\n",
    "        logits = self.linear(mean_embeds)  # スカラー\n",
    "\n",
    "        return logits  # このまま loss 関数に渡せる\n",
    "\n",
    "\n",
    "def eval_model(model, eval_data, device):\n",
    "    pred_labels = []\n",
    "    gold_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in eval_data:\n",
    "            input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "            label = sample[\"label\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "            prob = torch.sigmoid(logits)\n",
    "            pred = (prob >= 0.5).float()\n",
    "            pred_labels.append(pred.item())\n",
    "            gold_labels.append(label.item())\n",
    "\n",
    "    return accuracy_score(gold_labels, pred_labels)\n",
    "\n",
    "\n",
    "\n",
    "# 単語埋め込みの読み込み\n",
    "model = KeyedVectors.load_word2vec_format(\"../第6章：単語ベクトル/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "vocab_size = len(model.key_to_index)\n",
    "embedding_dim = model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "word_to_id = {\"<PAD>\": 0} \n",
    "id_to_word = {0: \"<PAD>\"}\n",
    "\n",
    "for i, word in enumerate(model.key_to_index, start=1):\n",
    "    embedding_matrix[i] = model[word]\n",
    "    word_to_id[word] = i\n",
    "    id_to_word[i] = word\n",
    "\n",
    "# データセット読み込み\n",
    "df_train = pd.read_csv(\"../第7章：機械学習/SST-2/train.tsv\", sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"../第7章：機械学習/SST-2/dev.tsv\", sep=\"\\t\")\n",
    "train_data = load_data(df_train, \"sentence\", \"label\")\n",
    "dev_data = load_data(df_dev, \"sentence\", \"label\")\n",
    "\n",
    "# モデルの読み込み\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BoWClassifier(embedding_matrix)\n",
    "model.load_state_dict(torch.load(\"model/bow_classifier_73.pth\"))\n",
    "model = model.to(device)\n",
    "\n",
    "# 評価\n",
    "accuracy = eval_model(model, dev_data, device)\n",
    "print(f\"開発セットにおける正解率：{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O08V9g0mcJwe",
   "metadata": {
    "id": "O08V9g0mcJwe"
   },
   "source": [
    "## 75. パディング\n",
    "\n",
    "複数の事例が与えられたとき、これらをまとめて一つのテンソル・オブジェクトで表現する関数`collate`を実装せよ。与えられた複数の事例のトークン列の長さが異なるときは、トークン列の長さが最も長いものに揃え、0番のトークンIDでパディングをせよ。さらに、トークン列の長さが長いものから順に、事例を並び替えよ。\n",
    "\n",
    "例えば、訓練データセットの冒頭の4事例が次のように表されているとき、\n",
    "\n",
    "```\n",
    "[{'text': 'hide new secretions from the parental units',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])},\n",
    " {'text': 'contains no wit , only labored gags',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])},\n",
    " {'text': 'that loves its characters and communicates something rather beautiful about human nature',\n",
    "  'label': tensor([1.]),\n",
    "  'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964])},\n",
    " {'text': 'remains utterly satisfied to remain the same throughout',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}]\n",
    "```\n",
    "\n",
    "`collate`関数を通した結果は以下のようになることが想定される。\n",
    "\n",
    "```\n",
    "{'input_ids': tensor([\n",
    "    [     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
    "    [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
    "    [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
    "    [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]),\n",
    " 'label': tensor([\n",
    "    [1.],\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [0.]])}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc7b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    12,  23441,    156, 337301,  19596,   1814,    412,  23017],\n",
      "        [    12,    693,      5,    133,    254,    336,   1194,      0],\n",
      "        [ 17848,  35389,    638,      0,      0,      0,      0,      0],\n",
      "        [ 12063,      0,      0,      0,      0,      0,      0,      0]]), 'label': tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def load_data(df, text_col_name, label_col_name):\n",
    "    dict_list = []\n",
    "    for text, label in zip(df[text_col_name], df[label_col_name]):\n",
    "        input_ids = [word_to_id[token] for token in text.split() if token in word_to_id]\n",
    "        if len(input_ids) > 0:\n",
    "            dict = {\n",
    "                \"text\": text,\n",
    "                \"label\": torch.tensor([float(label)]),\n",
    "                \"input_ids\": torch.tensor(input_ids),\n",
    "            }\n",
    "            dict_list.append(dict)\n",
    "    return dict_list\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 各サンプルのinput_idsの長さを取得\n",
    "    lengths = [len(sample[\"input_ids\"]) for sample in batch]\n",
    "\n",
    "    # 長さの降順でソートし、lengthsにおけるインデックスを記録\n",
    "    sorted_indices = np.argsort(lengths)[::-1]\n",
    "\n",
    "    sorted_batch = [batch[i] for i in sorted_indices]\n",
    "\n",
    "    input_ids_list = [sample[\"input_ids\"] for sample in sorted_batch]\n",
    "    labels = [sample[\"label\"] for sample in sorted_batch]\n",
    "\n",
    "    # 最長の長さ\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    # パディング：すべてのシーケンスをmax_lenに揃える（PAD=0）\n",
    "    padded_input_ids = []\n",
    "    for ids in input_ids_list:\n",
    "        pad_len = max_len - len(ids)\n",
    "        padded = torch.cat([ids, torch.zeros(pad_len, dtype=torch.long)])\n",
    "        padded_input_ids.append(padded)\n",
    "\n",
    "    # Tensorにまとめる\n",
    "    input_ids_tensor = torch.stack(padded_input_ids)  # (batch, max_len)\n",
    "    labels_tensor = torch.stack(labels)  # (batch, 1)\n",
    "\n",
    "    return {\"input_ids\": input_ids_tensor, \"label\": labels_tensor}\n",
    "\n",
    "\n",
    "\n",
    "# 単語埋め込みの読み込み\n",
    "model = KeyedVectors.load_word2vec_format(\"../第6章：単語ベクトル/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "vocab_size = len(model.key_to_index)\n",
    "embedding_dim = model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "word_to_id = {\"<PAD>\": 0} \n",
    "id_to_word = {0: \"<PAD>\"}\n",
    "\n",
    "for i, word in enumerate(model.key_to_index, start=1):\n",
    "    embedding_matrix[i] = model[word]\n",
    "    word_to_id[word] = i\n",
    "    id_to_word[i] = word\n",
    "    \n",
    "# データセット読み込み\n",
    "df_train = pd.read_csv(\"../第7章：機械学習/SST-2/train.tsv\", sep=\"\\t\")\n",
    "train_data = load_data(df_train, \"sentence\", \"label\")\n",
    "\n",
    "# DataLoaderの作成\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# バッチを1つ取得\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9NzvuZ-5ebDU",
   "metadata": {
    "id": "9NzvuZ-5ebDU"
   },
   "source": [
    "## 76. ミニバッチ学習\n",
    "\n",
    "問題75のパディングの処理を活用して、ミニバッチでモデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca8363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 591.87it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 617.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.6446, Train acc: 0.6928,\n",
      "---> Dev   loss: 0.6107, Dev   acc: 0.7018\n",
      "\n",
      "epoch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 598.65it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1177.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.5821, Train acc: 0.7562,\n",
      "---> Dev   loss: 0.5642, Dev   acc: 0.7592\n",
      "\n",
      "epoch3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 575.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1179.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.5422, Train acc: 0.7894,\n",
      "---> Dev   loss: 0.5354, Dev   acc: 0.7741\n",
      "\n",
      "epoch4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 545.74it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 603.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.5129, Train acc: 0.8061,\n",
      "---> Dev   loss: 0.5175, Dev   acc: 0.7764\n",
      "\n",
      "epoch5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 590.63it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1125.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4916, Train acc: 0.8173,\n",
      "---> Dev   loss: 0.5034, Dev   acc: 0.7764\n",
      "\n",
      "epoch6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 591.97it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1116.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4754, Train acc: 0.8212,\n",
      "---> Dev   loss: 0.4966, Dev   acc: 0.7775\n",
      "\n",
      "epoch7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 584.67it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1145.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4631, Train acc: 0.8266,\n",
      "---> Dev   loss: 0.4887, Dev   acc: 0.7833\n",
      "\n",
      "epoch8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 599.65it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1228.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4529, Train acc: 0.8283,\n",
      "---> Dev   loss: 0.4859, Dev   acc: 0.7798\n",
      "\n",
      "epoch9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 580.45it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1228.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4449, Train acc: 0.8305,\n",
      "---> Dev   loss: 0.4836, Dev   acc: 0.7821\n",
      "\n",
      "epoch10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 578.19it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1235.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4389, Train acc: 0.8307,\n",
      "---> Dev   loss: 0.4838, Dev   acc: 0.7844\n",
      "\n",
      "epoch11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 574.59it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1205.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4335, Train acc: 0.8330,\n",
      "---> Dev   loss: 0.4807, Dev   acc: 0.7890\n",
      "\n",
      "epoch12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 559.51it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1201.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4290, Train acc: 0.8340,\n",
      "---> Dev   loss: 0.4795, Dev   acc: 0.7856\n",
      "\n",
      "epoch13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 588.61it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 712.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4250, Train acc: 0.8345,\n",
      "---> Dev   loss: 0.4803, Dev   acc: 0.7856\n",
      "\n",
      "epoch14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 568.19it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 531.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4215, Train acc: 0.8354,\n",
      "---> Dev   loss: 0.4805, Dev   acc: 0.7844\n",
      "\n",
      "epoch15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 578.52it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1184.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4194, Train acc: 0.8362,\n",
      "---> Dev   loss: 0.4783, Dev   acc: 0.7856\n",
      "\n",
      "epoch16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 578.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 501.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4158, Train acc: 0.8363,\n",
      "---> Dev   loss: 0.4795, Dev   acc: 0.7878\n",
      "\n",
      "epoch17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 572.95it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 551.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4147, Train acc: 0.8376,\n",
      "---> Dev   loss: 0.4797, Dev   acc: 0.7878\n",
      "\n",
      "epoch18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 586.48it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1178.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4128, Train acc: 0.8378,\n",
      "---> Dev   loss: 0.4803, Dev   acc: 0.7878\n",
      "\n",
      "epoch19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 601.82it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1192.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4105, Train acc: 0.8383,\n",
      "---> Dev   loss: 0.4801, Dev   acc: 0.7890\n",
      "\n",
      "epoch20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 595.02it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1231.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4092, Train acc: 0.8388,\n",
      "---> Dev   loss: 0.4805, Dev   acc: 0.7878\n",
      "\n",
      "epoch21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 611.08it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1206.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4073, Train acc: 0.8390,\n",
      "---> Dev   loss: 0.4795, Dev   acc: 0.7890\n",
      "\n",
      "epoch22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 584.48it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1043.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4066, Train acc: 0.8393,\n",
      "---> Dev   loss: 0.4800, Dev   acc: 0.7890\n",
      "\n",
      "epoch23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 585.45it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 547.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4048, Train acc: 0.8400,\n",
      "---> Dev   loss: 0.4801, Dev   acc: 0.7890\n",
      "\n",
      "epoch24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 588.19it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1211.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4031, Train acc: 0.8404,\n",
      "---> Dev   loss: 0.4808, Dev   acc: 0.7901\n",
      "\n",
      "epoch25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 592.73it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1198.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4031, Train acc: 0.8405,\n",
      "---> Dev   loss: 0.4800, Dev   acc: 0.7924\n",
      "\n",
      "epoch26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 605.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 568.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4018, Train acc: 0.8403,\n",
      "---> Dev   loss: 0.4800, Dev   acc: 0.7924\n",
      "\n",
      "epoch27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 582.98it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 927.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4006, Train acc: 0.8404,\n",
      "---> Dev   loss: 0.4831, Dev   acc: 0.7913\n",
      "\n",
      "epoch28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 677.03it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1190.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4007, Train acc: 0.8412,\n",
      "---> Dev   loss: 0.4813, Dev   acc: 0.7901\n",
      "\n",
      "epoch29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 605.49it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1225.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3995, Train acc: 0.8421,\n",
      "---> Dev   loss: 0.4813, Dev   acc: 0.7901\n",
      "\n",
      "epoch30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 576.38it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1183.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3978, Train acc: 0.8408,\n",
      "---> Dev   loss: 0.4829, Dev   acc: 0.7936\n",
      "\n",
      "epoch31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 599.43it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1170.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3973, Train acc: 0.8419,\n",
      "---> Dev   loss: 0.4823, Dev   acc: 0.7913\n",
      "\n",
      "epoch32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 615.44it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 593.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3973, Train acc: 0.8421,\n",
      "---> Dev   loss: 0.4830, Dev   acc: 0.7936\n",
      "\n",
      "epoch33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 590.19it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 635.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3963, Train acc: 0.8421,\n",
      "---> Dev   loss: 0.4844, Dev   acc: 0.7924\n",
      "\n",
      "epoch34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 575.33it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1201.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3961, Train acc: 0.8426,\n",
      "---> Dev   loss: 0.4835, Dev   acc: 0.7936\n",
      "\n",
      "epoch35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 598.29it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 808.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3959, Train acc: 0.8425,\n",
      "---> Dev   loss: 0.4836, Dev   acc: 0.7936\n",
      "\n",
      "epoch36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 587.63it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1205.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3949, Train acc: 0.8424,\n",
      "---> Dev   loss: 0.4846, Dev   acc: 0.7924\n",
      "\n",
      "epoch37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 598.37it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1198.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3939, Train acc: 0.8433,\n",
      "---> Dev   loss: 0.4831, Dev   acc: 0.7959\n",
      "\n",
      "epoch38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 644.08it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1198.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3942, Train acc: 0.8426,\n",
      "---> Dev   loss: 0.4855, Dev   acc: 0.7947\n",
      "\n",
      "epoch39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 607.41it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1141.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3938, Train acc: 0.8432,\n",
      "---> Dev   loss: 0.4842, Dev   acc: 0.7947\n",
      "\n",
      "epoch40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 750.94it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1124.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3929, Train acc: 0.8434,\n",
      "---> Dev   loss: 0.4837, Dev   acc: 0.7947\n",
      "\n",
      "epoch41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 585.22it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1173.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3925, Train acc: 0.8427,\n",
      "---> Dev   loss: 0.4857, Dev   acc: 0.7947\n",
      "\n",
      "epoch42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 583.78it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1016.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3919, Train acc: 0.8429,\n",
      "---> Dev   loss: 0.4866, Dev   acc: 0.7959\n",
      "\n",
      "epoch43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 657.93it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1046.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3922, Train acc: 0.8436,\n",
      "---> Dev   loss: 0.4847, Dev   acc: 0.7982\n",
      "\n",
      "epoch44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 586.34it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1190.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3922, Train acc: 0.8441,\n",
      "---> Dev   loss: 0.4851, Dev   acc: 0.7993\n",
      "\n",
      "epoch45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 578.44it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1207.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3918, Train acc: 0.8435,\n",
      "---> Dev   loss: 0.4854, Dev   acc: 0.7993\n",
      "\n",
      "epoch46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 583.97it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 949.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3910, Train acc: 0.8438,\n",
      "---> Dev   loss: 0.4854, Dev   acc: 0.7993\n",
      "\n",
      "epoch47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 595.61it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 647.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3909, Train acc: 0.8442,\n",
      "---> Dev   loss: 0.4844, Dev   acc: 0.7970\n",
      "\n",
      "epoch48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 587.35it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 872.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3902, Train acc: 0.8428,\n",
      "---> Dev   loss: 0.4877, Dev   acc: 0.8016\n",
      "\n",
      "epoch49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 594.28it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 979.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3896, Train acc: 0.8438,\n",
      "---> Dev   loss: 0.4864, Dev   acc: 0.8005\n",
      "\n",
      "epoch50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [00:01<00:00, 578.27it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1211.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3896, Train acc: 0.8438,\n",
      "---> Dev   loss: 0.4860, Dev   acc: 0.8016\n",
      "\n",
      "\n",
      "モデルを保存しました: model/bow_classifier_76.pth\n",
      "開発セットにおける正解率：0.8016\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def load_data(df, text_col_name, label_col_name):\n",
    "    dict_list = []\n",
    "    for text, label in zip(df[text_col_name], df[label_col_name]):\n",
    "        input_ids = [word_to_id[token] for token in text.split() if token in word_to_id]\n",
    "        if len(input_ids) > 0:\n",
    "            dict = {\n",
    "                \"text\": text,\n",
    "                \"label\": torch.tensor([float(label)]),\n",
    "                \"input_ids\": torch.tensor(input_ids),\n",
    "            }\n",
    "            dict_list.append(dict)\n",
    "    return dict_list\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "\n",
    "        # 通常の nn.Embedding はランダムなベクトルから始まるが、from_pretrainedを使うことで、すでにWord2Vecから作ったベクトルを使う\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=True,  # 訓練中にこのベクトルを更新しない\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids : 形状 (batch, seq_len) または (seq_len, )\n",
    "        \"\"\"\n",
    "        # 単語ID → 単語ベクトル\n",
    "        embeds = self.embedding(input_ids)\n",
    "\n",
    "        # 単語ベクトルの平均\n",
    "        if len(embeds.shape) == 2:\n",
    "            mean_embeds = embeds.mean(dim=0)  # 1文のとき  embeds.shape = (seq_len, embedding_dim)\n",
    "        else:\n",
    "            mean_embeds = embeds.mean(dim=1)  # ミニバッチの場合  embeds.shape = (batch, seq_len, embedding_dim)\n",
    "\n",
    "        # 線形変換（ロジスティック回帰）\n",
    "        logits = self.linear(mean_embeds)  # スカラー\n",
    "\n",
    "        return logits  # このまま loss 関数に渡せる\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 各サンプルのinput_idsの長さを取得\n",
    "    lengths = [len(sample[\"input_ids\"]) for sample in batch]\n",
    "\n",
    "    # 長さの降順でソートし、lengthsにおけるインデックスを記録\n",
    "    sorted_indices = np.argsort(lengths)[::-1]\n",
    "\n",
    "    sorted_batch = [batch[i] for i in sorted_indices]\n",
    "\n",
    "    input_ids_list = [sample[\"input_ids\"] for sample in sorted_batch]\n",
    "    labels = [sample[\"label\"] for sample in sorted_batch]\n",
    "\n",
    "    # 最長の長さ\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    # パディング：すべてのシーケンスをmax_lenに揃える（PAD=0）\n",
    "    padded_input_ids = []\n",
    "    for ids in input_ids_list:\n",
    "        pad_len = max_len - len(ids)\n",
    "        padded = torch.cat([ids, torch.zeros(pad_len, dtype=torch.long)])\n",
    "        padded_input_ids.append(padded)\n",
    "\n",
    "    # Tensorにまとめる\n",
    "    input_ids_tensor = torch.stack(padded_input_ids)  # (batch, max_len)\n",
    "    labels_tensor = torch.stack(labels)  # (batch, 1)\n",
    "\n",
    "    return {\"input_ids\": input_ids_tensor, \"label\": labels_tensor}\n",
    "\n",
    "\n",
    "# 学習関数\n",
    "def train_batch_model(model, train_loader, dev_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", leave=True):\n",
    "        input_ids = batch[\"input_ids\"].to(device)  # (batch, seq_len)\n",
    "        labels = batch[\"label\"].to(device)  # (batch, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    dev_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_loader, desc=\"Validation\", leave=True):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            dev_losses.append(loss.item())\n",
    "\n",
    "    return np.mean(train_losses), np.mean(dev_losses)\n",
    "\n",
    "\n",
    "# 評価関数\n",
    "def eval_batch_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    pred_labels, gold_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            prob = torch.sigmoid(logits)\n",
    "            pred = (prob >= 0.5).float()\n",
    "\n",
    "            pred_labels.extend(pred.cpu().squeeze(1).tolist())\n",
    "            gold_labels.extend(labels.cpu().squeeze(1).tolist())\n",
    "\n",
    "    return accuracy_score(gold_labels, pred_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 単語埋め込みの読み込み\n",
    "model = KeyedVectors.load_word2vec_format(\"../第6章：単語ベクトル/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "vocab_size = len(model.key_to_index)\n",
    "embedding_dim = model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "word_to_id = {\"<PAD>\": 0} \n",
    "id_to_word = {0: \"<PAD>\"}\n",
    "\n",
    "for i, word in enumerate(model.key_to_index, start=1):\n",
    "    embedding_matrix[i] = model[word]\n",
    "    word_to_id[word] = i\n",
    "    id_to_word[i] = word\n",
    "\n",
    "# データセット読み込み\n",
    "df_train = pd.read_csv(\"../第7章：機械学習/SST-2/train.tsv\", sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"../第7章：機械学習/SST-2/dev.tsv\", sep=\"\\t\")\n",
    "train_data = load_data(df_train, \"sentence\", \"label\")\n",
    "dev_data = load_data(df_dev, \"sentence\", \"label\")\n",
    "\n",
    "# ハイパーパラメータと初期設定\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# モデル・損失関数・最適化手法\n",
    "model = BoWClassifier(embedding_matrix).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "parameters = model.linear.parameters()  # 単語埋め込みはfreeze=Trueなので、線形層だけが学習対象\n",
    "optimizer = optim.Adam(parameters, lr=learning_rate)\n",
    "\n",
    "# DataLoaderの作成\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 学習\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch{epoch+1}\")\n",
    "    train_loss, dev_loss = train_batch_model(\n",
    "        model, train_loader, dev_loader, criterion, optimizer, device\n",
    "    )\n",
    "\n",
    "    train_acc = eval_batch_model(model, train_loader, device)\n",
    "    dev_acc = eval_batch_model(model, dev_loader, device)\n",
    "\n",
    "    print(\n",
    "        f\"---> Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f},\\n\"\n",
    "        f\"---> Dev   loss: {dev_loss:.4f}, Dev   acc: {dev_acc:.4f}\\n\"\n",
    "    )\n",
    "\n",
    "# モデル保存\n",
    "save_path = \"model/bow_classifier_76.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"\\nモデルを保存しました: {save_path}\")\n",
    "\n",
    "# 評価\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model = model.to(device)\n",
    "accuracy = eval_batch_model(model, dev_loader, device)\n",
    "print(f\"開発セットにおける正解率：{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RUbjivUTejxn",
   "metadata": {
    "id": "RUbjivUTejxn"
   },
   "source": [
    "## 77. GPU上での学習\n",
    "\n",
    "問題76のモデル学習をGPU上で実行せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題76と同じ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZUY1PsD-eplq",
   "metadata": {
    "id": "ZUY1PsD-eplq"
   },
   "source": [
    "## 78. 単語埋め込みのファインチューニング\n",
    "\n",
    "問題77の学習において、単語埋め込みのパラメータも同時に更新するファインチューニングを導入せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016c251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.82it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1150.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.6744, Train acc: 0.6007,\n",
      "---> Dev   loss: 0.6676, Dev   acc: 0.5734\n",
      "\n",
      "epoch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.86it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1198.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.6298, Train acc: 0.7062,\n",
      "---> Dev   loss: 0.6060, Dev   acc: 0.7133\n",
      "\n",
      "epoch3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.86it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1104.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.5599, Train acc: 0.7848,\n",
      "---> Dev   loss: 0.5394, Dev   acc: 0.7557\n",
      "\n",
      "epoch4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.86it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1210.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4901, Train acc: 0.8348,\n",
      "---> Dev   loss: 0.4905, Dev   acc: 0.7867\n",
      "\n",
      "epoch5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.86it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1212.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4353, Train acc: 0.8518,\n",
      "---> Dev   loss: 0.4621, Dev   acc: 0.7936\n",
      "\n",
      "epoch6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.86it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1228.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3900, Train acc: 0.8801,\n",
      "---> Dev   loss: 0.4412, Dev   acc: 0.8050\n",
      "\n",
      "epoch7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.86it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1206.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3566, Train acc: 0.8876,\n",
      "---> Dev   loss: 0.4337, Dev   acc: 0.8131\n",
      "\n",
      "epoch8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.86it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1201.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3300, Train acc: 0.8899,\n",
      "---> Dev   loss: 0.4344, Dev   acc: 0.8108\n",
      "\n",
      "epoch9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.86it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1034.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3073, Train acc: 0.8993,\n",
      "---> Dev   loss: 0.4337, Dev   acc: 0.8142\n",
      "\n",
      "epoch10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.86it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1199.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.2886, Train acc: 0.9049,\n",
      "---> Dev   loss: 0.4373, Dev   acc: 0.8119\n",
      "\n",
      "\n",
      "モデルを保存しました: model/bow_classifier_76.pth\n",
      "開発セットにおける正解率：0.8119\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def load_data(df, text_col_name, label_col_name):\n",
    "    dict_list = []\n",
    "    for text, label in zip(df[text_col_name], df[label_col_name]):\n",
    "        input_ids = [word_to_id[token] for token in text.split() if token in word_to_id]\n",
    "        if len(input_ids) > 0:\n",
    "            dict = {\n",
    "                \"text\": text,\n",
    "                \"label\": torch.tensor([float(label)]),\n",
    "                \"input_ids\": torch.tensor(input_ids),\n",
    "            }\n",
    "            dict_list.append(dict)\n",
    "    return dict_list\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "\n",
    "        # 通常の nn.Embedding はランダムなベクトルから始まるが、from_pretrainedを使うことで、すでにWord2Vecから作ったベクトルを使う\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=False  # 単語埋め込みのパラメータも学習\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids : 形状 (batch, seq_len) または (seq_len, )\n",
    "        \"\"\"\n",
    "        # 単語ID → 単語ベクトル\n",
    "        embeds = self.embedding(input_ids)\n",
    "\n",
    "        # 単語ベクトルの平均\n",
    "        if len(embeds.shape) == 2:\n",
    "            mean_embeds = embeds.mean(dim=0)  # 1文のとき  embeds.shape = (seq_len, embedding_dim)\n",
    "        else:\n",
    "            mean_embeds = embeds.mean(dim=1)  # ミニバッチの場合  embeds.shape = (batch, seq_len, embedding_dim)\n",
    "\n",
    "        # 線形変換（ロジスティック回帰）\n",
    "        logits = self.linear(mean_embeds)  # スカラー\n",
    "\n",
    "        return logits  # このまま loss 関数に渡せる\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 各サンプルのinput_idsの長さを取得\n",
    "    lengths = [len(sample[\"input_ids\"]) for sample in batch]\n",
    "\n",
    "    # 長さの降順でソートし、lengthsにおけるインデックスを記録\n",
    "    sorted_indices = np.argsort(lengths)[::-1]\n",
    "\n",
    "    sorted_batch = [batch[i] for i in sorted_indices]\n",
    "\n",
    "    input_ids_list = [sample[\"input_ids\"] for sample in sorted_batch]\n",
    "    labels = [sample[\"label\"] for sample in sorted_batch]\n",
    "\n",
    "    # 最長の長さ\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    # パディング：すべてのシーケンスをmax_lenに揃える（PAD=0）\n",
    "    padded_input_ids = []\n",
    "    for ids in input_ids_list:\n",
    "        pad_len = max_len - len(ids)\n",
    "        padded = torch.cat([ids, torch.zeros(pad_len, dtype=torch.long)])\n",
    "        padded_input_ids.append(padded)\n",
    "\n",
    "    # Tensorにまとめる\n",
    "    input_ids_tensor = torch.stack(padded_input_ids)  # (batch, max_len)\n",
    "    labels_tensor = torch.stack(labels)  # (batch, 1)\n",
    "\n",
    "    return {\"input_ids\": input_ids_tensor, \"label\": labels_tensor}\n",
    "\n",
    "\n",
    "# 学習関数\n",
    "def train_batch_model(model, train_loader, dev_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", leave=True):\n",
    "        input_ids = batch[\"input_ids\"].to(device)  # (batch, seq_len)\n",
    "        labels = batch[\"label\"].to(device)  # (batch, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    dev_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_loader, desc=\"Validation\", leave=True):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            dev_losses.append(loss.item())\n",
    "\n",
    "    return np.mean(train_losses), np.mean(dev_losses)\n",
    "\n",
    "\n",
    "# 評価関数\n",
    "def eval_batch_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    pred_labels, gold_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            prob = torch.sigmoid(logits)\n",
    "            pred = (prob >= 0.5).float()\n",
    "\n",
    "            pred_labels.extend(pred.cpu().squeeze(1).tolist())\n",
    "            gold_labels.extend(labels.cpu().squeeze(1).tolist())\n",
    "\n",
    "    return accuracy_score(gold_labels, pred_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 単語埋め込みの読み込み\n",
    "model = KeyedVectors.load_word2vec_format(\"../第6章：単語ベクトル/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "vocab_size = len(model.key_to_index)\n",
    "embedding_dim = model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "word_to_id = {\"<PAD>\": 0} \n",
    "id_to_word = {0: \"<PAD>\"}\n",
    "\n",
    "for i, word in enumerate(model.key_to_index, start=1):\n",
    "    embedding_matrix[i] = model[word]\n",
    "    word_to_id[word] = i\n",
    "    id_to_word[i] = word\n",
    "\n",
    "# データセット読み込み\n",
    "df_train = pd.read_csv(\"../第7章：機械学習/SST-2/train.tsv\", sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"../第7章：機械学習/SST-2/dev.tsv\", sep=\"\\t\")\n",
    "train_data = load_data(df_train, \"sentence\", \"label\")\n",
    "dev_data = load_data(df_dev, \"sentence\", \"label\")\n",
    "\n",
    "# ハイパーパラメータと初期設定\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# モデル・損失関数・最適化手法\n",
    "model = BoWClassifier(embedding_matrix).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "parameters=model.parameters()  # 単語埋め込みのパラメータも学習\n",
    "optimizer = optim.Adam(parameters, lr=learning_rate)\n",
    "\n",
    "# DataLoaderの作成\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 学習\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch{epoch+1}\")\n",
    "    train_loss, dev_loss = train_batch_model(\n",
    "        model, train_loader, dev_loader, criterion, optimizer, device\n",
    "    )\n",
    "\n",
    "    train_acc = eval_batch_model(model, train_loader, device)\n",
    "    dev_acc = eval_batch_model(model, dev_loader, device)\n",
    "\n",
    "    print(\n",
    "        f\"---> Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f},\\n\"\n",
    "        f\"---> Dev   loss: {dev_loss:.4f}, Dev   acc: {dev_acc:.4f}\\n\"\n",
    "    )\n",
    "\n",
    "# モデル保存\n",
    "save_path = \"model/bow_classifier_76.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"\\nモデルを保存しました: {save_path}\")\n",
    "\n",
    "# 評価\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model = model.to(device)\n",
    "accuracy = eval_batch_model(model, dev_loader, device)\n",
    "print(f\"開発セットにおける正解率：{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jVAdWIq0evKR",
   "metadata": {
    "id": "jVAdWIq0evKR"
   },
   "source": [
    "## 79. アーキテクチャの変更\n",
    "\n",
    "ニューラルネットワークのアーキテクチャを自由に変更し、モデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。例えば、テキストの特徴ベクトル（単語埋め込みの平均ベクトル）に対して多層のニューラルネットワークを通したり、畳み込みニューラルネットワーク（CNN; Convolutional Neural Network）や再帰型ニューラルネットワーク（RNN; Recurrent Neural Network）などのモデルの学習に挑戦するとよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f5c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:15<00:00,  7.71it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1053.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.6920, Train acc: 0.5585,\n",
      "---> Dev   loss: 0.6899, Dev   acc: 0.5092\n",
      "\n",
      "epoch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:14<00:00,  7.75it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1033.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.6845, Train acc: 0.5651,\n",
      "---> Dev   loss: 0.6846, Dev   acc: 0.5149\n",
      "\n",
      "epoch3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1100.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.6759, Train acc: 0.5829,\n",
      "---> Dev   loss: 0.6770, Dev   acc: 0.5447\n",
      "\n",
      "epoch4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.84it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1096.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.6656, Train acc: 0.6052,\n",
      "---> Dev   loss: 0.6666, Dev   acc: 0.5883\n",
      "\n",
      "epoch5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1076.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.6541, Train acc: 0.6247,\n",
      "---> Dev   loss: 0.6538, Dev   acc: 0.6284\n",
      "\n",
      "epoch6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.84it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 984.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.6399, Train acc: 0.6498,\n",
      "---> Dev   loss: 0.6378, Dev   acc: 0.6686\n",
      "\n",
      "epoch7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.84it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1106.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.6232, Train acc: 0.6834,\n",
      "---> Dev   loss: 0.6189, Dev   acc: 0.7087\n",
      "\n",
      "epoch8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 433.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.6045, Train acc: 0.7047,\n",
      "---> Dev   loss: 0.5989, Dev   acc: 0.7282\n",
      "\n",
      "epoch9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.84it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1105.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.5838, Train acc: 0.7384,\n",
      "---> Dev   loss: 0.5775, Dev   acc: 0.7489\n",
      "\n",
      "epoch10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1105.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.5611, Train acc: 0.7653,\n",
      "---> Dev   loss: 0.5560, Dev   acc: 0.7683\n",
      "\n",
      "epoch11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.84it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1048.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.5388, Train acc: 0.7964,\n",
      "---> Dev   loss: 0.5350, Dev   acc: 0.7821\n",
      "\n",
      "epoch12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1111.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.5156, Train acc: 0.8148,\n",
      "---> Dev   loss: 0.5159, Dev   acc: 0.7878\n",
      "\n",
      "epoch13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.84it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 695.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4937, Train acc: 0.8333,\n",
      "---> Dev   loss: 0.4985, Dev   acc: 0.7936\n",
      "\n",
      "epoch14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1025.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4714, Train acc: 0.8452,\n",
      "---> Dev   loss: 0.4834, Dev   acc: 0.8073\n",
      "\n",
      "epoch15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1132.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4511, Train acc: 0.8543,\n",
      "---> Dev   loss: 0.4704, Dev   acc: 0.8062\n",
      "\n",
      "epoch16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1078.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4343, Train acc: 0.8600,\n",
      "---> Dev   loss: 0.4603, Dev   acc: 0.8085\n",
      "\n",
      "epoch17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1114.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4156, Train acc: 0.8711,\n",
      "---> Dev   loss: 0.4497, Dev   acc: 0.8165\n",
      "\n",
      "epoch18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1119.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.4009, Train acc: 0.8688,\n",
      "---> Dev   loss: 0.4454, Dev   acc: 0.8177\n",
      "\n",
      "epoch19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:12<00:00,  7.84it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1111.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3873, Train acc: 0.8775,\n",
      "---> Dev   loss: 0.4377, Dev   acc: 0.8234\n",
      "\n",
      "epoch20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1042/1042 [02:13<00:00,  7.83it/s]\n",
      "Validation: 100%|██████████| 14/14 [00:00<00:00, 1097.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train loss: 0.3739, Train acc: 0.8780,\n",
      "---> Dev   loss: 0.4351, Dev   acc: 0.8222\n",
      "\n",
      "\n",
      "モデルを保存しました: model/bow_classifier_79_MLP.pth\n",
      "開発セットにおける正解率：0.8222\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def load_data(df, text_col_name, label_col_name):\n",
    "    dict_list = []\n",
    "    for text, label in zip(df[text_col_name], df[label_col_name]):\n",
    "        input_ids = [word_to_id[token] for token in text.split() if token in word_to_id]\n",
    "        if len(input_ids) > 0:\n",
    "            dict = {\n",
    "                \"text\": text,\n",
    "                \"label\": torch.tensor([float(label)]),\n",
    "                \"input_ids\": torch.tensor(input_ids),\n",
    "            }\n",
    "            dict_list.append(dict)\n",
    "    return dict_list\n",
    "\n",
    "# 多層のニューラルネットワーク\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim=100, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=False  \n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids : 形状 (batch, seq_len) または (seq_len, )\n",
    "        \"\"\"\n",
    "        # 単語ID → 単語ベクトル\n",
    "        embeds = self.embedding(input_ids)\n",
    "\n",
    "        # 単語ベクトルの平均\n",
    "        if len(embeds.shape) == 2:\n",
    "            mean_embeds = embeds.mean(dim=0)  # 1文のとき  embeds.shape = (seq_len, embedding_dim)\n",
    "        else:\n",
    "            mean_embeds = embeds.mean(dim=1)  # ミニバッチの場合  embeds.shape = (batch, seq_len, embedding_dim)\n",
    "\n",
    "        # 線形変換（ロジスティック回帰）\n",
    "        logits = self.mlp(mean_embeds)  # スカラー\n",
    "\n",
    "        return logits  # このまま loss 関数に渡せる\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 各サンプルのinput_idsの長さを取得\n",
    "    lengths = [len(sample[\"input_ids\"]) for sample in batch]\n",
    "\n",
    "    # 長さの降順でソートし、lengthsにおけるインデックスを記録\n",
    "    sorted_indices = np.argsort(lengths)[::-1]\n",
    "\n",
    "    sorted_batch = [batch[i] for i in sorted_indices]\n",
    "\n",
    "    input_ids_list = [sample[\"input_ids\"] for sample in sorted_batch]\n",
    "    labels = [sample[\"label\"] for sample in sorted_batch]\n",
    "\n",
    "    # 最長の長さ\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    # パディング：すべてのシーケンスをmax_lenに揃える（PAD=0）\n",
    "    padded_input_ids = []\n",
    "    for ids in input_ids_list:\n",
    "        pad_len = max_len - len(ids)\n",
    "        padded = torch.cat([ids, torch.zeros(pad_len, dtype=torch.long)])\n",
    "        padded_input_ids.append(padded)\n",
    "\n",
    "    # Tensorにまとめる\n",
    "    input_ids_tensor = torch.stack(padded_input_ids)  # (batch, max_len)\n",
    "    labels_tensor = torch.stack(labels)  # (batch, 1)\n",
    "\n",
    "    return {\"input_ids\": input_ids_tensor, \"label\": labels_tensor}\n",
    "\n",
    "\n",
    "# 学習関数\n",
    "def train_batch_model(model, train_loader, dev_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", leave=True):\n",
    "        input_ids = batch[\"input_ids\"].to(device)  # (batch, seq_len)\n",
    "        labels = batch[\"label\"].to(device)  # (batch, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    dev_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_loader, desc=\"Validation\", leave=True):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            dev_losses.append(loss.item())\n",
    "\n",
    "    return np.mean(train_losses), np.mean(dev_losses)\n",
    "\n",
    "\n",
    "# 評価関数\n",
    "def eval_batch_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    pred_labels, gold_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            prob = torch.sigmoid(logits)\n",
    "            pred = (prob >= 0.5).float()\n",
    "\n",
    "            pred_labels.extend(pred.cpu().squeeze(1).tolist())\n",
    "            gold_labels.extend(labels.cpu().squeeze(1).tolist())\n",
    "\n",
    "    return accuracy_score(gold_labels, pred_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 単語埋め込みの読み込み\n",
    "model = KeyedVectors.load_word2vec_format(\"../第6章：単語ベクトル/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "vocab_size = len(model.key_to_index)\n",
    "embedding_dim = model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "word_to_id = {\"<PAD>\": 0} \n",
    "id_to_word = {0: \"<PAD>\"}\n",
    "\n",
    "for i, word in enumerate(model.key_to_index, start=1):\n",
    "    embedding_matrix[i] = model[word]\n",
    "    word_to_id[word] = i\n",
    "    id_to_word[i] = word\n",
    "\n",
    "# データセット読み込み\n",
    "df_train = pd.read_csv(\"../第7章：機械学習/SST-2/train.tsv\", sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"../第7章：機械学習/SST-2/dev.tsv\", sep=\"\\t\")\n",
    "train_data = load_data(df_train, \"sentence\", \"label\")\n",
    "dev_data = load_data(df_dev, \"sentence\", \"label\")\n",
    "\n",
    "# ハイパーパラメータと初期設定\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "learning_rate = 1e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# モデル・損失関数・最適化手法\n",
    "model = MLPClassifier(embedding_matrix).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "parameters=model.parameters()  # 単語埋め込みのパラメータも学習\n",
    "optimizer = optim.Adam(parameters, lr=learning_rate)\n",
    "\n",
    "# DataLoaderの作成\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 学習\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch{epoch+1}\")\n",
    "    train_loss, dev_loss = train_batch_model(\n",
    "        model, train_loader, dev_loader, criterion, optimizer, device\n",
    "    )\n",
    "\n",
    "    train_acc = eval_batch_model(model, train_loader, device)\n",
    "    dev_acc = eval_batch_model(model, dev_loader, device)\n",
    "\n",
    "    print(\n",
    "        f\"---> Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f},\\n\"\n",
    "        f\"---> Dev   loss: {dev_loss:.4f}, Dev   acc: {dev_acc:.4f}\\n\"\n",
    "    )\n",
    "\n",
    "# モデル保存\n",
    "save_path = \"model/bow_classifier_79_MLP.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"\\nモデルを保存しました: {save_path}\")\n",
    "\n",
    "# 評価\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model = model.to(device)\n",
    "accuracy = eval_batch_model(model, dev_loader, device)\n",
    "print(f\"開発セットにおける正解率：{accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp-100-knocks",
   "language": "python",
   "name": "nlp-100-knocks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
